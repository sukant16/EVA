{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 13.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D35vSs2M-Qw",
        "colab_type": "code",
        "outputId": "a423a829-9976-4294-b5c2-90cc70dad429",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w0YZt09M7-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/EVA\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSMAyBhzHaZm",
        "colab_type": "code",
        "outputId": "ed7dd7b9-7edd-4c1e-f897-9409b7c56281",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import np_utils    \n",
        "\n",
        "from deep_vision.resnet import ResNet\n",
        "from deep_vision.cutout import get_random_eraser\n",
        "from deep_vision.lr_finder import LR_Finder \n",
        "from deep_vision.clr_callback import CyclicLR"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hCCNCPkizKd",
        "colab_type": "text"
      },
      "source": [
        "Restrictions:\n",
        "- Your model must look like Conv->B1->B2->B3->B4 and not individually called Convs. \n",
        "- Batch Size 128\n",
        "- Normalization values of: (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
        "- Random Crop of 32 with padding of 4px Horizontal Flip (0.5)\n",
        "- Optimizer: SGD, Weight-Decay: 5e-4\n",
        "- NOT-OneCycleLR\n",
        "- Save model (to drive) after every 50 epochs or best model till now\n",
        "- Describe your blocks, and the stride strategy you have picked\n",
        "- Train for 300 Epochs\n",
        "- Assignment Target Accuracy is 90%, so exit gracefully if you reach 90% (you can target more, it can go till ~93%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OwxXNiaO6dm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128 #@param {type:\"integer\"}\n",
        "MOMENTUM = 0.9 #@param {type:\"number\"}\n",
        "WEIGHT_DECAY = 5e-4 #@param {type:\"number\"}\n",
        "EPOCHS = 300 #@param {type:\"integer\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zELf8wAiPKLP",
        "colab_type": "code",
        "outputId": "533977d3-5f79-43b4-f7d4-5f78bafbca7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "len_train, len_test = len(x_train), len(x_test)\n",
        "y_train = y_train.astype('int64').reshape(len_train)\n",
        "y_test = y_test.astype('int64').reshape(len_test)\n",
        "\n",
        "train_mean = np.mean(x_train, axis=(0,1,2))\n",
        "train_std = np.std(x_train, axis=(0,1,2))\n",
        "\n",
        "given_mean, given_std = (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "print(\"Calc mean:\", train_mean)\n",
        "print(\"Calc std:\", train_std) "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "Calc mean: [125.30691805 122.95039414 113.86538318]\n",
            "Calc std: [62.99321928 62.08870764 66.70489964]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtEMSE86O8lV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalize and padding the training data\n",
        "normalize = lambda x: ((x - given_mean) / given_std).astype('float32') # todo: check here\n",
        "pad4 = lambda x: np.pad(x, [(0, 0), (4, 4), (4, 4), (0, 0)], mode='reflect')\n",
        "\n",
        "x_train_norm = normalize(x_train)\n",
        "x_train = normalize(pad4(x_train))\n",
        "x_test = normalize(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1qjRG8JSG0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "norm_x_train_mean = np.mean(x_train_norm, axis=(0,1,2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fcG16X6RYtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXgcbUUXMycn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "As we training CIFAR-10, we will be using basic blocks where we are using \n",
        "fully pre-activated BN-RELU-CONV(3x3) as  per ResNet V2.  \n",
        "Resnet.py contains the entire code for building the model\n",
        "''' \n",
        "def make_model():\n",
        "    return ResNet(input_shape=(32,32,3), classes=10, block='basic', residual_unit='v2',\n",
        "                  repetitions=[2,2,2,2], initial_filters=64, activation='softmax', include_top=True,\n",
        "                  input_tensor=None, dropout=None, transition_dilation_rate=(1, 1),\n",
        "                  initial_strides=(1, 1), initial_kernel_size=(3, 3), initial_pooling=None,\n",
        "                  final_pooling='avg', top='classification')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMHcPHq9ZU_F",
        "colab_type": "text"
      },
      "source": [
        "#### Finding the min and max learning rates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk9NKddBO0XX",
        "colab_type": "code",
        "outputId": "4f9d5f78-a6bd-48ba-d5ed-4357ca1e9d1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "source": [
        "model = make_model()\n",
        "sgd = SGD(1e-4, momentum=MOMENTUM, nesterov=True, decay=WEIGHT_DECAY)\n",
        "\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "reshaping via a convolution...\n",
            "reshaping via a convolution...\n",
            "reshaping via a convolution...\n",
            "reshaping via a convolution...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EFXp1p_Ku_0",
        "colab_type": "code",
        "outputId": "92e50487-8dfc-4e4d-eb78-6290bbd0d6a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "lr_finder = LR_Finder(start_lr=1e-7, end_lr=10, step_size=np.ceil(x_train_norm.shape[0]/BATCH_SIZE))\n",
        "model.fit(x_train_norm, Y_train, callbacks=[lr_finder], batch_size=BATCH_SIZE)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/1\n",
            "50000/50000 [==============================] - 25s 501us/step - loss: 3.3897 - acc: 0.1927\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f97e0164da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jMP81NNSpdC",
        "colab_type": "code",
        "outputId": "4e6fc9e9-c1cb-415a-8e8a-547bb524609e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "lr_finder.plot_lr()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc2uEShQTDqz",
        "colab_type": "code",
        "outputId": "4a388833-8a0f-459e-ca07-90ae5dc8bc65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "lr_finder.plot_lr_loss()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf9z0ptpAAadTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5Ya7UMTTLrm",
        "colab_type": "code",
        "outputId": "6734079c-af7a-423b-d924-d3d1148e753c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "lr_finder.plot_smoothed_lr_loss()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAENCAYAAAACHGKEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIC8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQcx8KjyTQ9q",
        "colab_type": "text"
      },
      "source": [
        "So, from the above plot, we can see that the min and max bounds of learning rate which are 0.0001 and 0.1 respectively"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJW4IFrYQBRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for random cropping\n",
        "def random_crop(img, random_crop_size):\n",
        "    # Note: image_data_format is 'channel_last'\n",
        "    assert img.shape[2] == 3\n",
        "    height, width = img.shape[0], img.shape[1]\n",
        "    dy, dx = random_crop_size\n",
        "    x = np.random.randint(0, width - dx + 1)\n",
        "    y = np.random.randint(0, height - dy + 1)\n",
        "    return img[y:(y+dy), x:(x+dx), :]\n",
        "\n",
        "\n",
        "def crop_generator(batches, crop_length):\n",
        "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
        "    crops from the image batches generated by the original iterator.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        batch_x, batch_y = next(batches)\n",
        "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
        "        for i in range(batch_x.shape[0]):\n",
        "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
        "        yield (batch_crops, batch_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTUVo3YHT3H0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the minimum learning rate, maximum learning rate, batch size, step size, CLR method, and number of epochs\n",
        "# Triangular2 is the methods where the max learning rate tapers of linearly in each cycle\n",
        "MIN_LR = 0.0001\n",
        "MAX_LR = 0.1\n",
        "STEP_SIZE = 8\n",
        "CLR_METHOD = \"triangular2\"\n",
        "NUM_EPOCHS = 96"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9n6NAcwZndN",
        "colab_type": "text"
      },
      "source": [
        "### Traininf the mode with cyclic LR\n",
        "\n",
        "Code for cyclic LR can be found in clr_callback.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQwuLFaQT3Fv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "8d12b139-a1a8-4e28-a45e-c594466cc2cb"
      },
      "source": [
        "model = make_model()\n",
        "sgd = SGD(MIN_LR, momentum=MOMENTUM, nesterov=True, decay=WEIGHT_DECAY)\n",
        "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "clr = CyclicLR(mode=CLR_METHOD,\n",
        "               base_lr=MIN_LR,\n",
        "               max_lr=MAX_LR,\n",
        "               step_size= STEP_SIZE * (x_train.shape[0] // BATCH_SIZE))\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=\"best_model.h5\", \n",
        "                             save_best_only=True,\n",
        "                             period=50,\n",
        "                             monitor=\"val_loss\",\n",
        "                             verbose=1)\n",
        "\n",
        "earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=2, verbose=0, mode='auto', baseline=90.0, restore_best_weights=True)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reshaping via a convolution...\n",
            "reshaping via a convolution...\n",
            "reshaping via a convolution...\n",
            "reshaping via a convolution...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdA1Qd8mLPr5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b3eb5a7d-7d78-4acd-fb7d-456f54e026bc"
      },
      "source": [
        "# random_crop = lambda x, y: (tf.image.random_flip_left_right(tf.random_crop(x, [32, 32, 3])), y)\n",
        "\n",
        "datagen = ImageDataGenerator(preprocessing_function=get_random_eraser(pixel_value=norm_x_train_mean), \n",
        "                             horizontal_flip=True)\n",
        "\n",
        "train_batches = datagen.flow(x_train, Y_train, batch_size=BATCH_SIZE)\n",
        "train_cropped_batches = crop_generator(train_batches, 32)\n",
        "\n",
        "model.fit_generator(train_cropped_batches,\n",
        "                    steps_per_epoch=x_train.shape[0]//BATCH_SIZE, \n",
        "                    epochs=EPOCHS, \n",
        "                    validation_data=(x_test, Y_test), \n",
        "                    callbacks=[clr, checkpoint, earlystop],\n",
        "                    verbose=1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "390/390 [==============================] - 27s 70ms/step - loss: 2.7379 - acc: 0.3523 - val_loss: 2.5363 - val_acc: 0.4337\n",
            "Epoch 2/300\n",
            "  3/390 [..............................] - ETA: 21s - loss: 2.5277 - acc: 0.4365"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:842: RuntimeWarning: Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: val_loss,val_acc,loss,acc\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "390/390 [==============================] - 24s 63ms/step - loss: 2.2942 - acc: 0.5156 - val_loss: 2.1437 - val_acc: 0.5712\n",
            "Epoch 3/300\n",
            "390/390 [==============================] - 25s 63ms/step - loss: 2.0713 - acc: 0.5939 - val_loss: 2.1959 - val_acc: 0.5569\n",
            "Epoch 4/300\n",
            "390/390 [==============================] - 24s 61ms/step - loss: 1.9094 - acc: 0.6453 - val_loss: 2.0879 - val_acc: 0.6107\n",
            "Epoch 5/300\n",
            "390/390 [==============================] - 24s 62ms/step - loss: 1.7752 - acc: 0.6825 - val_loss: 2.5292 - val_acc: 0.5425\n",
            "Epoch 6/300\n",
            "390/390 [==============================] - 25s 64ms/step - loss: 1.6690 - acc: 0.7158 - val_loss: 1.7069 - val_acc: 0.7002\n",
            "Epoch 7/300\n",
            "390/390 [==============================] - 24s 62ms/step - loss: 1.5755 - acc: 0.7378 - val_loss: 2.1579 - val_acc: 0.5855\n",
            "Epoch 8/300\n",
            "390/390 [==============================] - 24s 60ms/step - loss: 1.4944 - acc: 0.7539 - val_loss: 2.1184 - val_acc: 0.6065\n",
            "Epoch 9/300\n",
            "390/390 [==============================] - 24s 62ms/step - loss: 1.4097 - acc: 0.7750 - val_loss: 1.5767 - val_acc: 0.7258\n",
            "Epoch 10/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 1.3163 - acc: 0.7966 - val_loss: 1.3005 - val_acc: 0.7971\n",
            "Epoch 11/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 1.2459 - acc: 0.8141 - val_loss: 1.3186 - val_acc: 0.7949\n",
            "Epoch 12/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 1.1839 - acc: 0.8272 - val_loss: 1.1827 - val_acc: 0.8282\n",
            "Epoch 13/300\n",
            "390/390 [==============================] - 24s 60ms/step - loss: 1.1347 - acc: 0.8417 - val_loss: 1.1199 - val_acc: 0.8526\n",
            "Epoch 14/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 1.0840 - acc: 0.8568 - val_loss: 1.0927 - val_acc: 0.8540\n",
            "Epoch 15/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 1.0499 - acc: 0.8652 - val_loss: 1.0389 - val_acc: 0.8739\n",
            "Epoch 16/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 1.0266 - acc: 0.8724 - val_loss: 1.0073 - val_acc: 0.8842\n",
            "Epoch 17/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 1.0146 - acc: 0.8785 - val_loss: 1.0160 - val_acc: 0.8806\n",
            "Epoch 18/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 1.0110 - acc: 0.8779 - val_loss: 1.0351 - val_acc: 0.8755\n",
            "Epoch 19/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 1.0136 - acc: 0.8768 - val_loss: 1.0454 - val_acc: 0.8729\n",
            "Epoch 20/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 1.0122 - acc: 0.8753 - val_loss: 1.0264 - val_acc: 0.8789\n",
            "Epoch 21/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 1.0088 - acc: 0.8755 - val_loss: 1.0435 - val_acc: 0.8671\n",
            "Epoch 22/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 1.0044 - acc: 0.8761 - val_loss: 1.1414 - val_acc: 0.8397\n",
            "Epoch 23/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.9984 - acc: 0.8744 - val_loss: 1.0370 - val_acc: 0.8662\n",
            "Epoch 24/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.9949 - acc: 0.8726 - val_loss: 1.1111 - val_acc: 0.8394\n",
            "Epoch 25/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.9764 - acc: 0.8763 - val_loss: 1.0300 - val_acc: 0.8617\n",
            "Epoch 26/300\n",
            "390/390 [==============================] - 24s 60ms/step - loss: 0.9558 - acc: 0.8817 - val_loss: 0.9981 - val_acc: 0.8774\n",
            "Epoch 27/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.9325 - acc: 0.8880 - val_loss: 0.9868 - val_acc: 0.8780\n",
            "Epoch 28/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.9148 - acc: 0.8924 - val_loss: 0.9568 - val_acc: 0.8848\n",
            "Epoch 29/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.8973 - acc: 0.8969 - val_loss: 0.9533 - val_acc: 0.8851\n",
            "Epoch 30/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.8860 - acc: 0.9021 - val_loss: 0.9279 - val_acc: 0.8926\n",
            "Epoch 31/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.8748 - acc: 0.9053 - val_loss: 0.9231 - val_acc: 0.8929\n",
            "Epoch 32/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.8613 - acc: 0.9100 - val_loss: 0.9145 - val_acc: 0.8975\n",
            "Epoch 33/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.8593 - acc: 0.9085 - val_loss: 0.9164 - val_acc: 0.8967\n",
            "Epoch 34/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.8605 - acc: 0.9100 - val_loss: 0.9161 - val_acc: 0.8966\n",
            "Epoch 35/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.8613 - acc: 0.9098 - val_loss: 0.9217 - val_acc: 0.8940\n",
            "Epoch 36/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.8634 - acc: 0.9077 - val_loss: 0.9276 - val_acc: 0.8943\n",
            "Epoch 37/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.8588 - acc: 0.9083 - val_loss: 0.9192 - val_acc: 0.8962\n",
            "Epoch 38/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.8559 - acc: 0.9098 - val_loss: 0.9186 - val_acc: 0.8964\n",
            "Epoch 39/300\n",
            "390/390 [==============================] - 24s 60ms/step - loss: 0.8550 - acc: 0.9093 - val_loss: 0.9351 - val_acc: 0.8885\n",
            "Epoch 40/300\n",
            "390/390 [==============================] - 24s 60ms/step - loss: 0.8535 - acc: 0.9077 - val_loss: 0.9230 - val_acc: 0.8931\n",
            "Epoch 41/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.8476 - acc: 0.9088 - val_loss: 0.9208 - val_acc: 0.8932\n",
            "Epoch 42/300\n",
            "390/390 [==============================] - 25s 63ms/step - loss: 0.8385 - acc: 0.9141 - val_loss: 0.9126 - val_acc: 0.8949\n",
            "Epoch 43/300\n",
            "390/390 [==============================] - 24s 61ms/step - loss: 0.8313 - acc: 0.9139 - val_loss: 0.9229 - val_acc: 0.8917\n",
            "Epoch 44/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.8296 - acc: 0.9147 - val_loss: 0.9043 - val_acc: 0.8970\n",
            "Epoch 45/300\n",
            "390/390 [==============================] - 24s 61ms/step - loss: 0.8228 - acc: 0.9161 - val_loss: 0.8982 - val_acc: 0.8969\n",
            "Epoch 46/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.8143 - acc: 0.9194 - val_loss: 0.8940 - val_acc: 0.9001\n",
            "Epoch 47/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.8125 - acc: 0.9210 - val_loss: 0.8940 - val_acc: 0.8975\n",
            "Epoch 48/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.8130 - acc: 0.9198 - val_loss: 0.8917 - val_acc: 0.9004\n",
            "Epoch 49/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.8120 - acc: 0.9205 - val_loss: 0.8912 - val_acc: 0.9005\n",
            "Epoch 50/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.8085 - acc: 0.9215 - val_loss: 0.8925 - val_acc: 0.9003\n",
            "\n",
            "Epoch 00050: val_loss improved from inf to 0.89246, saving model to best_model.h5\n",
            "Epoch 51/300\n",
            "390/390 [==============================] - 24s 60ms/step - loss: 0.8075 - acc: 0.9218 - val_loss: 0.8939 - val_acc: 0.9006\n",
            "Epoch 52/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.8078 - acc: 0.9208 - val_loss: 0.8952 - val_acc: 0.9001\n",
            "Epoch 53/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.8065 - acc: 0.9205 - val_loss: 0.8964 - val_acc: 0.8990\n",
            "Epoch 54/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.8069 - acc: 0.9203 - val_loss: 0.8929 - val_acc: 0.9007\n",
            "Epoch 55/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.8013 - acc: 0.9225 - val_loss: 0.8973 - val_acc: 0.8976\n",
            "Epoch 56/300\n",
            "390/390 [==============================] - 24s 62ms/step - loss: 0.8044 - acc: 0.9214 - val_loss: 0.8939 - val_acc: 0.8992\n",
            "Epoch 57/300\n",
            "390/390 [==============================] - 24s 62ms/step - loss: 0.8009 - acc: 0.9220 - val_loss: 0.8953 - val_acc: 0.8971\n",
            "Epoch 58/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.8004 - acc: 0.9214 - val_loss: 0.8952 - val_acc: 0.8976\n",
            "Epoch 59/300\n",
            "390/390 [==============================] - 24s 60ms/step - loss: 0.7944 - acc: 0.9231 - val_loss: 0.8900 - val_acc: 0.8997\n",
            "Epoch 60/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7910 - acc: 0.9245 - val_loss: 0.8903 - val_acc: 0.9001\n",
            "Epoch 61/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7947 - acc: 0.9234 - val_loss: 0.8846 - val_acc: 0.9001\n",
            "Epoch 62/300\n",
            "390/390 [==============================] - 24s 61ms/step - loss: 0.7893 - acc: 0.9241 - val_loss: 0.8862 - val_acc: 0.9000\n",
            "Epoch 63/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7843 - acc: 0.9268 - val_loss: 0.8850 - val_acc: 0.9013\n",
            "Epoch 64/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7817 - acc: 0.9282 - val_loss: 0.8841 - val_acc: 0.9002\n",
            "Epoch 65/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7863 - acc: 0.9260 - val_loss: 0.8844 - val_acc: 0.9000\n",
            "Epoch 66/300\n",
            "390/390 [==============================] - 24s 61ms/step - loss: 0.7882 - acc: 0.9258 - val_loss: 0.8844 - val_acc: 0.9006\n",
            "Epoch 67/300\n",
            "390/390 [==============================] - 24s 63ms/step - loss: 0.7848 - acc: 0.9265 - val_loss: 0.8860 - val_acc: 0.9017\n",
            "Epoch 68/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7852 - acc: 0.9276 - val_loss: 0.8844 - val_acc: 0.8994\n",
            "Epoch 69/300\n",
            "390/390 [==============================] - 24s 60ms/step - loss: 0.7862 - acc: 0.9265 - val_loss: 0.8838 - val_acc: 0.9013\n",
            "Epoch 70/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7826 - acc: 0.9281 - val_loss: 0.8851 - val_acc: 0.9005\n",
            "Epoch 71/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7850 - acc: 0.9253 - val_loss: 0.8833 - val_acc: 0.9024\n",
            "Epoch 72/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7833 - acc: 0.9262 - val_loss: 0.8859 - val_acc: 0.8995\n",
            "Epoch 73/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7820 - acc: 0.9275 - val_loss: 0.8843 - val_acc: 0.9002\n",
            "Epoch 74/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7803 - acc: 0.9281 - val_loss: 0.8832 - val_acc: 0.9018\n",
            "Epoch 75/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7790 - acc: 0.9282 - val_loss: 0.8842 - val_acc: 0.8998\n",
            "Epoch 76/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7809 - acc: 0.9288 - val_loss: 0.8826 - val_acc: 0.9005\n",
            "Epoch 77/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7810 - acc: 0.9267 - val_loss: 0.8824 - val_acc: 0.9002\n",
            "Epoch 78/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7791 - acc: 0.9273 - val_loss: 0.8816 - val_acc: 0.9011\n",
            "Epoch 79/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7798 - acc: 0.9286 - val_loss: 0.8811 - val_acc: 0.9006\n",
            "Epoch 80/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7806 - acc: 0.9278 - val_loss: 0.8813 - val_acc: 0.9003\n",
            "Epoch 81/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7812 - acc: 0.9282 - val_loss: 0.8813 - val_acc: 0.9006\n",
            "Epoch 82/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7803 - acc: 0.9271 - val_loss: 0.8816 - val_acc: 0.9006\n",
            "Epoch 83/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7755 - acc: 0.9300 - val_loss: 0.8816 - val_acc: 0.9014\n",
            "Epoch 84/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7783 - acc: 0.9276 - val_loss: 0.8810 - val_acc: 0.9009\n",
            "Epoch 85/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7736 - acc: 0.9294 - val_loss: 0.8815 - val_acc: 0.9012\n",
            "Epoch 86/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7777 - acc: 0.9293 - val_loss: 0.8816 - val_acc: 0.9006\n",
            "Epoch 87/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7769 - acc: 0.9276 - val_loss: 0.8812 - val_acc: 0.9014\n",
            "Epoch 88/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7720 - acc: 0.9300 - val_loss: 0.8810 - val_acc: 0.9004\n",
            "Epoch 89/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7759 - acc: 0.9280 - val_loss: 0.8806 - val_acc: 0.9004\n",
            "Epoch 90/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7745 - acc: 0.9288 - val_loss: 0.8817 - val_acc: 0.9004\n",
            "Epoch 91/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7747 - acc: 0.9286 - val_loss: 0.8800 - val_acc: 0.9014\n",
            "Epoch 92/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7752 - acc: 0.9280 - val_loss: 0.8796 - val_acc: 0.9007\n",
            "Epoch 93/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7738 - acc: 0.9295 - val_loss: 0.8801 - val_acc: 0.9000\n",
            "Epoch 94/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7749 - acc: 0.9289 - val_loss: 0.8801 - val_acc: 0.9004\n",
            "Epoch 95/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7752 - acc: 0.9289 - val_loss: 0.8798 - val_acc: 0.8999\n",
            "Epoch 96/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7694 - acc: 0.9313 - val_loss: 0.8804 - val_acc: 0.8995\n",
            "Epoch 97/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7727 - acc: 0.9294 - val_loss: 0.8798 - val_acc: 0.9001\n",
            "Epoch 98/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7728 - acc: 0.9305 - val_loss: 0.8798 - val_acc: 0.9003\n",
            "Epoch 99/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7752 - acc: 0.9281 - val_loss: 0.8794 - val_acc: 0.8999\n",
            "Epoch 100/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7732 - acc: 0.9288 - val_loss: 0.8794 - val_acc: 0.9004\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.89246 to 0.87940, saving model to best_model.h5\n",
            "Epoch 101/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7718 - acc: 0.9299 - val_loss: 0.8791 - val_acc: 0.9003\n",
            "Epoch 102/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7754 - acc: 0.9298 - val_loss: 0.8802 - val_acc: 0.9006\n",
            "Epoch 103/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7704 - acc: 0.9295 - val_loss: 0.8798 - val_acc: 0.9006\n",
            "Epoch 104/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7702 - acc: 0.9292 - val_loss: 0.8804 - val_acc: 0.9002\n",
            "Epoch 105/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7708 - acc: 0.9307 - val_loss: 0.8801 - val_acc: 0.9005\n",
            "Epoch 106/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7717 - acc: 0.9295 - val_loss: 0.8803 - val_acc: 0.8994\n",
            "Epoch 107/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7707 - acc: 0.9307 - val_loss: 0.8796 - val_acc: 0.9004\n",
            "Epoch 108/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7690 - acc: 0.9305 - val_loss: 0.8793 - val_acc: 0.9003\n",
            "Epoch 109/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7729 - acc: 0.9300 - val_loss: 0.8797 - val_acc: 0.9009\n",
            "Epoch 110/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7689 - acc: 0.9310 - val_loss: 0.8796 - val_acc: 0.9010\n",
            "Epoch 111/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7708 - acc: 0.9300 - val_loss: 0.8799 - val_acc: 0.8999\n",
            "Epoch 112/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7685 - acc: 0.9309 - val_loss: 0.8794 - val_acc: 0.9008\n",
            "Epoch 113/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7688 - acc: 0.9313 - val_loss: 0.8795 - val_acc: 0.9002\n",
            "Epoch 114/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7733 - acc: 0.9286 - val_loss: 0.8798 - val_acc: 0.9010\n",
            "Epoch 115/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7710 - acc: 0.9300 - val_loss: 0.8796 - val_acc: 0.9003\n",
            "Epoch 116/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7750 - acc: 0.9276 - val_loss: 0.8798 - val_acc: 0.9005\n",
            "Epoch 117/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7696 - acc: 0.9313 - val_loss: 0.8800 - val_acc: 0.9004\n",
            "Epoch 118/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7696 - acc: 0.9319 - val_loss: 0.8795 - val_acc: 0.9015\n",
            "Epoch 119/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7700 - acc: 0.9297 - val_loss: 0.8799 - val_acc: 0.9014\n",
            "Epoch 120/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7698 - acc: 0.9307 - val_loss: 0.8799 - val_acc: 0.9010\n",
            "Epoch 121/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7723 - acc: 0.9296 - val_loss: 0.8793 - val_acc: 0.9009\n",
            "Epoch 122/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7711 - acc: 0.9309 - val_loss: 0.8797 - val_acc: 0.9006\n",
            "Epoch 123/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7713 - acc: 0.9306 - val_loss: 0.8794 - val_acc: 0.9009\n",
            "Epoch 124/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7747 - acc: 0.9287 - val_loss: 0.8799 - val_acc: 0.9004\n",
            "Epoch 125/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7687 - acc: 0.9306 - val_loss: 0.8791 - val_acc: 0.9004\n",
            "Epoch 126/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7655 - acc: 0.9311 - val_loss: 0.8791 - val_acc: 0.9006\n",
            "Epoch 127/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7723 - acc: 0.9297 - val_loss: 0.8794 - val_acc: 0.9006\n",
            "Epoch 128/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7704 - acc: 0.9299 - val_loss: 0.8793 - val_acc: 0.9009\n",
            "Epoch 129/300\n",
            "390/390 [==============================] - 24s 61ms/step - loss: 0.7684 - acc: 0.9313 - val_loss: 0.8793 - val_acc: 0.9005\n",
            "Epoch 130/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7786 - acc: 0.9262 - val_loss: 0.8795 - val_acc: 0.9006\n",
            "Epoch 131/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7711 - acc: 0.9306 - val_loss: 0.8788 - val_acc: 0.9006\n",
            "Epoch 132/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7720 - acc: 0.9310 - val_loss: 0.8793 - val_acc: 0.9009\n",
            "Epoch 133/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7715 - acc: 0.9294 - val_loss: 0.8793 - val_acc: 0.9013\n",
            "Epoch 134/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7727 - acc: 0.9295 - val_loss: 0.8790 - val_acc: 0.9007\n",
            "Epoch 135/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7745 - acc: 0.9289 - val_loss: 0.8795 - val_acc: 0.9005\n",
            "Epoch 136/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7669 - acc: 0.9330 - val_loss: 0.8792 - val_acc: 0.9005\n",
            "Epoch 137/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7705 - acc: 0.9304 - val_loss: 0.8789 - val_acc: 0.9010\n",
            "Epoch 138/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7727 - acc: 0.9299 - val_loss: 0.8799 - val_acc: 0.8999\n",
            "Epoch 139/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7744 - acc: 0.9282 - val_loss: 0.8792 - val_acc: 0.9004\n",
            "Epoch 140/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7739 - acc: 0.9289 - val_loss: 0.8793 - val_acc: 0.9005\n",
            "Epoch 141/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7699 - acc: 0.9298 - val_loss: 0.8793 - val_acc: 0.9006\n",
            "Epoch 142/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7676 - acc: 0.9308 - val_loss: 0.8792 - val_acc: 0.9007\n",
            "Epoch 143/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7688 - acc: 0.9301 - val_loss: 0.8788 - val_acc: 0.9007\n",
            "Epoch 144/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7666 - acc: 0.9314 - val_loss: 0.8787 - val_acc: 0.9006\n",
            "Epoch 145/300\n",
            "390/390 [==============================] - 24s 60ms/step - loss: 0.7684 - acc: 0.9313 - val_loss: 0.8788 - val_acc: 0.9007\n",
            "Epoch 146/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7717 - acc: 0.9293 - val_loss: 0.8789 - val_acc: 0.9004\n",
            "Epoch 147/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7721 - acc: 0.9295 - val_loss: 0.8791 - val_acc: 0.9008\n",
            "Epoch 148/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7682 - acc: 0.9310 - val_loss: 0.8791 - val_acc: 0.9009\n",
            "Epoch 149/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7707 - acc: 0.9289 - val_loss: 0.8787 - val_acc: 0.9007\n",
            "Epoch 150/300\n",
            "390/390 [==============================] - 24s 61ms/step - loss: 0.7699 - acc: 0.9297 - val_loss: 0.8790 - val_acc: 0.9009\n",
            "\n",
            "Epoch 00150: val_loss improved from 0.87940 to 0.87898, saving model to best_model.h5\n",
            "Epoch 151/300\n",
            "390/390 [==============================] - 24s 61ms/step - loss: 0.7743 - acc: 0.9287 - val_loss: 0.8793 - val_acc: 0.9007\n",
            "Epoch 152/300\n",
            "390/390 [==============================] - 24s 62ms/step - loss: 0.7668 - acc: 0.9312 - val_loss: 0.8788 - val_acc: 0.9002\n",
            "Epoch 153/300\n",
            "390/390 [==============================] - 24s 61ms/step - loss: 0.7709 - acc: 0.9303 - val_loss: 0.8792 - val_acc: 0.9007\n",
            "Epoch 154/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7670 - acc: 0.9312 - val_loss: 0.8789 - val_acc: 0.9009\n",
            "Epoch 155/300\n",
            "390/390 [==============================] - 24s 61ms/step - loss: 0.7673 - acc: 0.9316 - val_loss: 0.8789 - val_acc: 0.9008\n",
            "Epoch 156/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7676 - acc: 0.9298 - val_loss: 0.8789 - val_acc: 0.9011\n",
            "Epoch 157/300\n",
            "390/390 [==============================] - 24s 61ms/step - loss: 0.7698 - acc: 0.9294 - val_loss: 0.8786 - val_acc: 0.9002\n",
            "Epoch 158/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7697 - acc: 0.9315 - val_loss: 0.8791 - val_acc: 0.9007\n",
            "Epoch 159/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7677 - acc: 0.9312 - val_loss: 0.8789 - val_acc: 0.9005\n",
            "Epoch 160/300\n",
            "390/390 [==============================] - 24s 60ms/step - loss: 0.7718 - acc: 0.9300 - val_loss: 0.8788 - val_acc: 0.9012\n",
            "Epoch 161/300\n",
            "390/390 [==============================] - 24s 61ms/step - loss: 0.7687 - acc: 0.9312 - val_loss: 0.8794 - val_acc: 0.9006\n",
            "Epoch 162/300\n",
            "390/390 [==============================] - 25s 63ms/step - loss: 0.7694 - acc: 0.9309 - val_loss: 0.8791 - val_acc: 0.9010\n",
            "Epoch 163/300\n",
            "390/390 [==============================] - 24s 61ms/step - loss: 0.7695 - acc: 0.9312 - val_loss: 0.8789 - val_acc: 0.9011\n",
            "Epoch 164/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7692 - acc: 0.9307 - val_loss: 0.8790 - val_acc: 0.9010\n",
            "Epoch 165/300\n",
            "390/390 [==============================] - 24s 61ms/step - loss: 0.7721 - acc: 0.9300 - val_loss: 0.8789 - val_acc: 0.9008\n",
            "Epoch 166/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7755 - acc: 0.9282 - val_loss: 0.8791 - val_acc: 0.9010\n",
            "Epoch 167/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7735 - acc: 0.9288 - val_loss: 0.8790 - val_acc: 0.9009\n",
            "Epoch 168/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7674 - acc: 0.9323 - val_loss: 0.8790 - val_acc: 0.9002\n",
            "Epoch 169/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7712 - acc: 0.9312 - val_loss: 0.8788 - val_acc: 0.9006\n",
            "Epoch 170/300\n",
            "390/390 [==============================] - 24s 60ms/step - loss: 0.7674 - acc: 0.9316 - val_loss: 0.8792 - val_acc: 0.9009\n",
            "Epoch 171/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7658 - acc: 0.9310 - val_loss: 0.8790 - val_acc: 0.9012\n",
            "Epoch 172/300\n",
            "390/390 [==============================] - 24s 60ms/step - loss: 0.7703 - acc: 0.9295 - val_loss: 0.8789 - val_acc: 0.9004\n",
            "Epoch 173/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7725 - acc: 0.9293 - val_loss: 0.8792 - val_acc: 0.9012\n",
            "Epoch 174/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7687 - acc: 0.9307 - val_loss: 0.8791 - val_acc: 0.9008\n",
            "Epoch 175/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7719 - acc: 0.9291 - val_loss: 0.8786 - val_acc: 0.9010\n",
            "Epoch 176/300\n",
            "390/390 [==============================] - 24s 60ms/step - loss: 0.7692 - acc: 0.9297 - val_loss: 0.8791 - val_acc: 0.9008\n",
            "Epoch 177/300\n",
            "390/390 [==============================] - 24s 60ms/step - loss: 0.7701 - acc: 0.9304 - val_loss: 0.8790 - val_acc: 0.9008\n",
            "Epoch 178/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7637 - acc: 0.9328 - val_loss: 0.8787 - val_acc: 0.9010\n",
            "Epoch 179/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7706 - acc: 0.9309 - val_loss: 0.8788 - val_acc: 0.9015\n",
            "Epoch 180/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7689 - acc: 0.9306 - val_loss: 0.8787 - val_acc: 0.9006\n",
            "Epoch 181/300\n",
            "390/390 [==============================] - 24s 60ms/step - loss: 0.7699 - acc: 0.9295 - val_loss: 0.8788 - val_acc: 0.9007\n",
            "Epoch 182/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7700 - acc: 0.9303 - val_loss: 0.8791 - val_acc: 0.9009\n",
            "Epoch 183/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7734 - acc: 0.9288 - val_loss: 0.8790 - val_acc: 0.9009\n",
            "Epoch 184/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7683 - acc: 0.9313 - val_loss: 0.8790 - val_acc: 0.9004\n",
            "Epoch 185/300\n",
            "390/390 [==============================] - 24s 60ms/step - loss: 0.7703 - acc: 0.9297 - val_loss: 0.8785 - val_acc: 0.9014\n",
            "Epoch 186/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7693 - acc: 0.9310 - val_loss: 0.8788 - val_acc: 0.9007\n",
            "Epoch 187/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7695 - acc: 0.9313 - val_loss: 0.8787 - val_acc: 0.9011\n",
            "Epoch 188/300\n",
            "390/390 [==============================] - 24s 60ms/step - loss: 0.7689 - acc: 0.9323 - val_loss: 0.8789 - val_acc: 0.9011\n",
            "Epoch 189/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7688 - acc: 0.9313 - val_loss: 0.8787 - val_acc: 0.9008\n",
            "Epoch 190/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7720 - acc: 0.9298 - val_loss: 0.8787 - val_acc: 0.9009\n",
            "Epoch 191/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7674 - acc: 0.9311 - val_loss: 0.8788 - val_acc: 0.9009\n",
            "Epoch 192/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7690 - acc: 0.9301 - val_loss: 0.8786 - val_acc: 0.9008\n",
            "Epoch 193/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7714 - acc: 0.9300 - val_loss: 0.8786 - val_acc: 0.9011\n",
            "Epoch 194/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7696 - acc: 0.9310 - val_loss: 0.8786 - val_acc: 0.9009\n",
            "Epoch 195/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7699 - acc: 0.9312 - val_loss: 0.8788 - val_acc: 0.9006\n",
            "Epoch 196/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7715 - acc: 0.9292 - val_loss: 0.8789 - val_acc: 0.9015\n",
            "Epoch 197/300\n",
            "390/390 [==============================] - 24s 61ms/step - loss: 0.7710 - acc: 0.9301 - val_loss: 0.8787 - val_acc: 0.9009\n",
            "Epoch 198/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7739 - acc: 0.9281 - val_loss: 0.8792 - val_acc: 0.9007\n",
            "Epoch 199/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7675 - acc: 0.9307 - val_loss: 0.8785 - val_acc: 0.9010\n",
            "Epoch 200/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7682 - acc: 0.9299 - val_loss: 0.8789 - val_acc: 0.9011\n",
            "\n",
            "Epoch 00200: val_loss improved from 0.87898 to 0.87888, saving model to best_model.h5\n",
            "Epoch 201/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7686 - acc: 0.9309 - val_loss: 0.8787 - val_acc: 0.9006\n",
            "Epoch 202/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7701 - acc: 0.9294 - val_loss: 0.8791 - val_acc: 0.9007\n",
            "Epoch 203/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7637 - acc: 0.9330 - val_loss: 0.8784 - val_acc: 0.9013\n",
            "Epoch 204/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7695 - acc: 0.9312 - val_loss: 0.8788 - val_acc: 0.9011\n",
            "Epoch 205/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7710 - acc: 0.9301 - val_loss: 0.8785 - val_acc: 0.9011\n",
            "Epoch 206/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7684 - acc: 0.9308 - val_loss: 0.8784 - val_acc: 0.9014\n",
            "Epoch 207/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7713 - acc: 0.9303 - val_loss: 0.8788 - val_acc: 0.9005\n",
            "Epoch 208/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7701 - acc: 0.9302 - val_loss: 0.8786 - val_acc: 0.9010\n",
            "Epoch 209/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7697 - acc: 0.9309 - val_loss: 0.8788 - val_acc: 0.9011\n",
            "Epoch 210/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7753 - acc: 0.9269 - val_loss: 0.8785 - val_acc: 0.9010\n",
            "Epoch 211/300\n",
            "390/390 [==============================] - 24s 61ms/step - loss: 0.7669 - acc: 0.9307 - val_loss: 0.8788 - val_acc: 0.9013\n",
            "Epoch 212/300\n",
            "390/390 [==============================] - 24s 61ms/step - loss: 0.7714 - acc: 0.9301 - val_loss: 0.8788 - val_acc: 0.9008\n",
            "Epoch 213/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7692 - acc: 0.9303 - val_loss: 0.8785 - val_acc: 0.9009\n",
            "Epoch 214/300\n",
            "390/390 [==============================] - 24s 62ms/step - loss: 0.7731 - acc: 0.9301 - val_loss: 0.8788 - val_acc: 0.9010\n",
            "Epoch 215/300\n",
            "390/390 [==============================] - 24s 61ms/step - loss: 0.7672 - acc: 0.9317 - val_loss: 0.8783 - val_acc: 0.9008\n",
            "Epoch 216/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7722 - acc: 0.9291 - val_loss: 0.8786 - val_acc: 0.9012\n",
            "Epoch 217/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7699 - acc: 0.9310 - val_loss: 0.8790 - val_acc: 0.9014\n",
            "Epoch 218/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7685 - acc: 0.9304 - val_loss: 0.8788 - val_acc: 0.9011\n",
            "Epoch 219/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7652 - acc: 0.9325 - val_loss: 0.8786 - val_acc: 0.9012\n",
            "Epoch 220/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7725 - acc: 0.9293 - val_loss: 0.8789 - val_acc: 0.9012\n",
            "Epoch 221/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7689 - acc: 0.9303 - val_loss: 0.8788 - val_acc: 0.9010\n",
            "Epoch 222/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7651 - acc: 0.9324 - val_loss: 0.8790 - val_acc: 0.9011\n",
            "Epoch 223/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7682 - acc: 0.9304 - val_loss: 0.8789 - val_acc: 0.9008\n",
            "Epoch 224/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7666 - acc: 0.9322 - val_loss: 0.8790 - val_acc: 0.9009\n",
            "Epoch 225/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7667 - acc: 0.9308 - val_loss: 0.8784 - val_acc: 0.9007\n",
            "Epoch 226/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7691 - acc: 0.9304 - val_loss: 0.8784 - val_acc: 0.9007\n",
            "Epoch 227/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7707 - acc: 0.9296 - val_loss: 0.8791 - val_acc: 0.9011\n",
            "Epoch 228/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7686 - acc: 0.9317 - val_loss: 0.8787 - val_acc: 0.9014\n",
            "Epoch 229/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7671 - acc: 0.9313 - val_loss: 0.8786 - val_acc: 0.9014\n",
            "Epoch 230/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7683 - acc: 0.9304 - val_loss: 0.8790 - val_acc: 0.9006\n",
            "Epoch 231/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7675 - acc: 0.9312 - val_loss: 0.8785 - val_acc: 0.9009\n",
            "Epoch 232/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7679 - acc: 0.9311 - val_loss: 0.8786 - val_acc: 0.9020\n",
            "Epoch 233/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7684 - acc: 0.9314 - val_loss: 0.8788 - val_acc: 0.9011\n",
            "Epoch 234/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7667 - acc: 0.9312 - val_loss: 0.8787 - val_acc: 0.9010\n",
            "Epoch 235/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7675 - acc: 0.9304 - val_loss: 0.8787 - val_acc: 0.9012\n",
            "Epoch 236/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7737 - acc: 0.9305 - val_loss: 0.8786 - val_acc: 0.9010\n",
            "Epoch 237/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7704 - acc: 0.9306 - val_loss: 0.8791 - val_acc: 0.9015\n",
            "Epoch 238/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7637 - acc: 0.9326 - val_loss: 0.8786 - val_acc: 0.9009\n",
            "Epoch 239/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7681 - acc: 0.9299 - val_loss: 0.8789 - val_acc: 0.9010\n",
            "Epoch 240/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7693 - acc: 0.9305 - val_loss: 0.8788 - val_acc: 0.9011\n",
            "Epoch 241/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7671 - acc: 0.9312 - val_loss: 0.8788 - val_acc: 0.9012\n",
            "Epoch 242/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7700 - acc: 0.9303 - val_loss: 0.8783 - val_acc: 0.9010\n",
            "Epoch 243/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7704 - acc: 0.9310 - val_loss: 0.8787 - val_acc: 0.9008\n",
            "Epoch 244/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7674 - acc: 0.9312 - val_loss: 0.8786 - val_acc: 0.9014\n",
            "Epoch 245/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7694 - acc: 0.9301 - val_loss: 0.8784 - val_acc: 0.9011\n",
            "Epoch 246/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7671 - acc: 0.9323 - val_loss: 0.8787 - val_acc: 0.9009\n",
            "Epoch 247/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7692 - acc: 0.9309 - val_loss: 0.8785 - val_acc: 0.9013\n",
            "Epoch 248/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7659 - acc: 0.9318 - val_loss: 0.8785 - val_acc: 0.9007\n",
            "Epoch 249/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7665 - acc: 0.9309 - val_loss: 0.8788 - val_acc: 0.9005\n",
            "Epoch 250/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7704 - acc: 0.9291 - val_loss: 0.8788 - val_acc: 0.9006\n",
            "\n",
            "Epoch 00250: val_loss improved from 0.87888 to 0.87884, saving model to best_model.h5\n",
            "Epoch 251/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7727 - acc: 0.9290 - val_loss: 0.8782 - val_acc: 0.9014\n",
            "Epoch 252/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7729 - acc: 0.9283 - val_loss: 0.8787 - val_acc: 0.9005\n",
            "Epoch 253/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7685 - acc: 0.9304 - val_loss: 0.8785 - val_acc: 0.9012\n",
            "Epoch 254/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7685 - acc: 0.9312 - val_loss: 0.8788 - val_acc: 0.9014\n",
            "Epoch 255/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7647 - acc: 0.9320 - val_loss: 0.8785 - val_acc: 0.9016\n",
            "Epoch 256/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7708 - acc: 0.9291 - val_loss: 0.8787 - val_acc: 0.9009\n",
            "Epoch 257/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7663 - acc: 0.9321 - val_loss: 0.8785 - val_acc: 0.9010\n",
            "Epoch 258/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7684 - acc: 0.9299 - val_loss: 0.8786 - val_acc: 0.9010\n",
            "Epoch 259/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7718 - acc: 0.9302 - val_loss: 0.8789 - val_acc: 0.9009\n",
            "Epoch 260/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7661 - acc: 0.9323 - val_loss: 0.8786 - val_acc: 0.9008\n",
            "Epoch 261/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7683 - acc: 0.9302 - val_loss: 0.8784 - val_acc: 0.9009\n",
            "Epoch 262/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7687 - acc: 0.9308 - val_loss: 0.8784 - val_acc: 0.9010\n",
            "Epoch 263/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7711 - acc: 0.9288 - val_loss: 0.8785 - val_acc: 0.9014\n",
            "Epoch 264/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7675 - acc: 0.9308 - val_loss: 0.8786 - val_acc: 0.9005\n",
            "Epoch 265/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7696 - acc: 0.9298 - val_loss: 0.8787 - val_acc: 0.9010\n",
            "Epoch 266/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7702 - acc: 0.9297 - val_loss: 0.8784 - val_acc: 0.9011\n",
            "Epoch 267/300\n",
            "390/390 [==============================] - 24s 62ms/step - loss: 0.7692 - acc: 0.9292 - val_loss: 0.8787 - val_acc: 0.9012\n",
            "Epoch 268/300\n",
            "390/390 [==============================] - 24s 62ms/step - loss: 0.7696 - acc: 0.9292 - val_loss: 0.8785 - val_acc: 0.9008\n",
            "Epoch 269/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7691 - acc: 0.9307 - val_loss: 0.8789 - val_acc: 0.9007\n",
            "Epoch 270/300\n",
            "390/390 [==============================] - 24s 60ms/step - loss: 0.7682 - acc: 0.9314 - val_loss: 0.8785 - val_acc: 0.9008\n",
            "Epoch 271/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7705 - acc: 0.9305 - val_loss: 0.8788 - val_acc: 0.9009\n",
            "Epoch 272/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7673 - acc: 0.9308 - val_loss: 0.8786 - val_acc: 0.9008\n",
            "Epoch 273/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7711 - acc: 0.9295 - val_loss: 0.8787 - val_acc: 0.9009\n",
            "Epoch 274/300\n",
            "390/390 [==============================] - 24s 60ms/step - loss: 0.7679 - acc: 0.9307 - val_loss: 0.8788 - val_acc: 0.9016\n",
            "Epoch 275/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7685 - acc: 0.9317 - val_loss: 0.8786 - val_acc: 0.9007\n",
            "Epoch 276/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7696 - acc: 0.9295 - val_loss: 0.8787 - val_acc: 0.9011\n",
            "Epoch 277/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7710 - acc: 0.9303 - val_loss: 0.8784 - val_acc: 0.9008\n",
            "Epoch 278/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7702 - acc: 0.9301 - val_loss: 0.8786 - val_acc: 0.9007\n",
            "Epoch 279/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7683 - acc: 0.9313 - val_loss: 0.8787 - val_acc: 0.9007\n",
            "Epoch 280/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7677 - acc: 0.9301 - val_loss: 0.8787 - val_acc: 0.9003\n",
            "Epoch 281/300\n",
            "390/390 [==============================] - 24s 60ms/step - loss: 0.7662 - acc: 0.9317 - val_loss: 0.8787 - val_acc: 0.9016\n",
            "Epoch 282/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7678 - acc: 0.9321 - val_loss: 0.8783 - val_acc: 0.9008\n",
            "Epoch 283/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7725 - acc: 0.9299 - val_loss: 0.8786 - val_acc: 0.9011\n",
            "Epoch 284/300\n",
            "390/390 [==============================] - 24s 60ms/step - loss: 0.7692 - acc: 0.9301 - val_loss: 0.8787 - val_acc: 0.9008\n",
            "Epoch 285/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7647 - acc: 0.9322 - val_loss: 0.8787 - val_acc: 0.9010\n",
            "Epoch 286/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7655 - acc: 0.9325 - val_loss: 0.8789 - val_acc: 0.9008\n",
            "Epoch 287/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7702 - acc: 0.9311 - val_loss: 0.8785 - val_acc: 0.9008\n",
            "Epoch 288/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7709 - acc: 0.9290 - val_loss: 0.8789 - val_acc: 0.9010\n",
            "Epoch 289/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7663 - acc: 0.9315 - val_loss: 0.8786 - val_acc: 0.9014\n",
            "Epoch 290/300\n",
            "390/390 [==============================] - 24s 61ms/step - loss: 0.7666 - acc: 0.9302 - val_loss: 0.8785 - val_acc: 0.9010\n",
            "Epoch 291/300\n",
            "390/390 [==============================] - 24s 62ms/step - loss: 0.7717 - acc: 0.9297 - val_loss: 0.8787 - val_acc: 0.9008\n",
            "Epoch 292/300\n",
            "390/390 [==============================] - 24s 62ms/step - loss: 0.7706 - acc: 0.9293 - val_loss: 0.8787 - val_acc: 0.9006\n",
            "Epoch 293/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7676 - acc: 0.9302 - val_loss: 0.8786 - val_acc: 0.9009\n",
            "Epoch 294/300\n",
            "390/390 [==============================] - 24s 60ms/step - loss: 0.7706 - acc: 0.9298 - val_loss: 0.8785 - val_acc: 0.9009\n",
            "Epoch 295/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7699 - acc: 0.9302 - val_loss: 0.8784 - val_acc: 0.9007\n",
            "Epoch 296/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7721 - acc: 0.9293 - val_loss: 0.8788 - val_acc: 0.9011\n",
            "Epoch 297/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7706 - acc: 0.9294 - val_loss: 0.8787 - val_acc: 0.9007\n",
            "Epoch 298/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7620 - acc: 0.9324 - val_loss: 0.8785 - val_acc: 0.9010\n",
            "Epoch 299/300\n",
            "390/390 [==============================] - 23s 59ms/step - loss: 0.7660 - acc: 0.9308 - val_loss: 0.8787 - val_acc: 0.9005\n",
            "Epoch 300/300\n",
            "390/390 [==============================] - 23s 60ms/step - loss: 0.7669 - acc: 0.9317 - val_loss: 0.8789 - val_acc: 0.9007\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.87884\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9667c5ec50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpC5QEYwZ22Y",
        "colab_type": "text"
      },
      "source": [
        "We can see that model reached 90% accuracy quite early, however, it didn't go much above than that.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXpThjwvJlfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}